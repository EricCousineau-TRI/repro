{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db3e1ca0",
   "metadata": {},
   "source": [
    "Watered down setup + test for trivial \"energy\" net (which is just normal distribution).\n",
    "\n",
    "Check neighboring `./requirements.txt` for deps.\n",
    "\n",
    "Context:\n",
    " - <https://github.com/google-research/ibc/issues/6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52210b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import einops\n",
    "import matplotlib.pyplot as plt\n",
    "import moviepy\n",
    "import moviepy.editor as mpy\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd.functional import jacobian\n",
    "from torch.distributions.uniform import Uniform\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "DimY = 2\n",
    "y_min = torch.tensor([0.0, 0.0])\n",
    "y_max = torch.tensor([1.0, 1.0])\n",
    "step_size_init = 1e-3\n",
    "step_size_final = 1e-3\n",
    "step_size_power = 2\n",
    "n_iters = 100\n",
    "num_samples = 128\n",
    "\n",
    "\n",
    "def seed(value):\n",
    "    random.seed(value)\n",
    "    np.random.seed(value)\n",
    "    torch.manual_seed(value)\n",
    "\n",
    "\n",
    "def get_step_size(iteration):\n",
    "    blend = iteration / (n_iters - 1)\n",
    "    blend = blend ** step_size_power\n",
    "    step_size = step_size_init + blend * (step_size_final - step_size_init)\n",
    "    return step_size\n",
    "\n",
    "\n",
    "def uniform_sample(num_samples, batch_size):\n",
    "    lb = y_min.expand(batch_size, num_samples, DimY)\n",
    "    ub = y_max.expand(batch_size, num_samples, DimY)\n",
    "    return Uniform(lb, ub).sample()\n",
    "\n",
    "\n",
    "def langevin_step(y_samples, dE_dys, step_size, use_ibc_style=True):\n",
    "    # Independent draw for covariance.\n",
    "    y_noise = torch.normal(\n",
    "        mean=0.0,\n",
    "        std=1.0,\n",
    "        size=y_samples.shape,\n",
    "        device=y_samples.device,\n",
    "    )\n",
    "\n",
    "    # Perturb samples according to gradient and desired noise level.\n",
    "    if use_ibc_style:\n",
    "        # From google-research/ibc.\n",
    "        delta_y = -step_size * (0.5 * dE_dys + y_noise)\n",
    "    else:\n",
    "        # Better formulation. See:\n",
    "        # https://en.wikipedia.org/wiki/Metropolis-adjusted_Langevin_algorithm  # noqa\n",
    "        delta_y = -step_size * dE_dys + np.sqrt(2 * step_size) * y_noise\n",
    "\n",
    "    # Shift current actions\n",
    "    y_samples = y_samples + delta_y\n",
    "\n",
    "    return y_samples\n",
    "\n",
    "\n",
    "def gradient_wrt_act(ebm_net, x, ys):\n",
    "    \"\"\"Same as in google-research/ibc.\"\"\"\n",
    "    assert not torch.is_grad_enabled()\n",
    "\n",
    "    def Ex_sum(ys):\n",
    "        # Adapt trick from:\n",
    "        # https://discuss.pytorch.org/t/computing-batch-jacobian-efficiently/80771/5  # noqa\n",
    "        energies = ebm_net(x, ys)\n",
    "        return energies.sum()\n",
    "\n",
    "    # WARNING: This may be rather slow.\n",
    "    with torch.set_grad_enabled(True):\n",
    "        dE_dys = jacobian(Ex_sum, ys)\n",
    "    assert dE_dys.shape == ys.shape\n",
    "    return dE_dys\n",
    "\n",
    "\n",
    "def langevin_sample(ebm_net, x, num_samples, callback=None, *, use_ibc_style=True):\n",
    "    assert not torch.is_grad_enabled()\n",
    "    N = x.shape[0]\n",
    "\n",
    "    # Draw initial samples.\n",
    "    y_samples = uniform_sample(num_samples, batch_size=N)\n",
    "\n",
    "    for i in range(n_iters + 1):\n",
    "        is_last = i == n_iters\n",
    "\n",
    "        if callback is not None:\n",
    "            callback(i, y_samples)\n",
    "\n",
    "        if not is_last:\n",
    "            # Compute gradient.\n",
    "            dE_dys = gradient_wrt_act(ebm_net, x, y_samples)\n",
    "            # Compute step size given current iteration.\n",
    "            step_size = get_step_size(i)\n",
    "            # Produce next set of samples (driving towards typical set).\n",
    "            y_samples = langevin_step(y_samples, dE_dys, step_size, use_ibc_style=use_ibc_style)\n",
    "\n",
    "    return y_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e899f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_log_normal_pdf(x, mu, std):\n",
    "    \"\"\"Log of multivariate normal distribution.\"\"\"\n",
    "    N, k = x.shape\n",
    "    assert mu.shape == (k,)\n",
    "    assert mu.shape == std.shape\n",
    "    cov = torch.diag(std ** 2)\n",
    "    inv_cov = torch.diag(1 / std ** 2)\n",
    "    center = (x - mu).unsqueeze(-1)\n",
    "    weighted = -0.5 * center.transpose(1, 2) @ inv_cov @ center\n",
    "    log_denom = (2 * np.pi) * (k / 2) + std.sum()\n",
    "    out = weighted - log_denom\n",
    "    out = out.reshape(N)\n",
    "    return out\n",
    "\n",
    "\n",
    "class NormalEbm(nn.Module):\n",
    "    \"\"\"\n",
    "    Provides an energy function that represents a normal distribution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, y_mu, y_std):\n",
    "        super().__init__()\n",
    "        assert y_std.shape == y_mu.shape\n",
    "        self._y_std = y_std\n",
    "        self._y_mu = y_mu\n",
    "\n",
    "    def forward(self, x, ys):\n",
    "        N, K, _ = ys.shape\n",
    "        ys = einops.rearrange(ys, \"N K DimY -> (N K) DimY\")\n",
    "        # N.B. Incoprorate log into pdf computation so we avoid numeric\n",
    "        # problems in autograd.\n",
    "        probs = torch_log_normal_pdf(ys, self._y_mu, self._y_std)\n",
    "        probs = einops.rearrange(probs, \"(N K) -> N K 1\", N=N)\n",
    "        energies = -probs\n",
    "        return energies\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def plot_2d_ebm(ebm_net, x, grid_size=50, alpha=None, temperature=1.0):\n",
    "    action_x = torch.linspace(y_min[0], y_max[0], steps=grid_size)\n",
    "    action_y = torch.linspace(y_min[1], y_max[1], steps=grid_size)\n",
    "    action_y_grid, action_x_grid = torch.meshgrid(action_x, action_y)\n",
    "\n",
    "    action_xs = einops.rearrange(action_x_grid, \"H W -> (H W)\")\n",
    "    action_ys = einops.rearrange(action_y_grid, \"H W -> (H W)\")\n",
    "    ys = einops.rearrange([action_xs, action_ys], \"C HW -> () HW C\").to(x)\n",
    "    Zs = ebm_net(x, ys)\n",
    "\n",
    "    Z_grid = einops.rearrange(Zs, \"1 N 1 -> N\")\n",
    "    # Show probabilities used for sampling.\n",
    "    num_grid_samples = grid_size ** 2\n",
    "    Z_grid = F.softmax(-Z_grid / temperature, dim=0)\n",
    "    Z_grid = F.normalize(Z_grid, dim=0)\n",
    "    Z_grid = einops.rearrange(Z_grid, \"(H W) -> H W\", H=grid_size)\n",
    "    mesh = plt.pcolormesh(\n",
    "        action_x_grid.numpy(),\n",
    "        action_y_grid.numpy(),\n",
    "        Z_grid.numpy(),\n",
    "        cmap=\"magma\",\n",
    "        alpha=alpha,\n",
    "        shading=\"auto\",\n",
    "    )\n",
    "    return mesh\n",
    "\n",
    "\n",
    "def mpl_figure_to_image(fig):\n",
    "    fig.canvas.draw()\n",
    "    data = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "    W, H = fig.canvas.get_width_height()\n",
    "    C = 3\n",
    "    data = data.reshape((H, W, C))\n",
    "    return data\n",
    "\n",
    "\n",
    "def plot_2d_ebm_callback(ebm_net, iteration, x, y_samples):\n",
    "    N, _ = x.shape\n",
    "    assert N == 1\n",
    "    fig, ax = plt.subplots()\n",
    "    mesh = plot_2d_ebm(ebm_net, x)\n",
    "    plt.colorbar(mesh)\n",
    "    plt.grid(False)\n",
    "    plt.axis(\"scaled\")\n",
    "    y_samples = y_samples.squeeze(0)\n",
    "    plt.scatter(\n",
    "        y_samples[:, 0], y_samples[:, 1], marker=\"x\", s=10, color=\"green\"\n",
    "    )\n",
    "    plt.title(f\"Iter {iteration} / {n_iters}\")\n",
    "\n",
    "    image = mpl_figure_to_image(fig)\n",
    "    plt.close(fig)\n",
    "    return image\n",
    "\n",
    "\n",
    "def _repeat_last(images, *, count=5):\n",
    "    # To ensure last frame is saved.\n",
    "    assert len(images) > 0\n",
    "    return images + [images[-1]] * count\n",
    "\n",
    "\n",
    "def display_movie(clip):\n",
    "    return moviepy.video.io.html_tools.ipython_display(\n",
    "        clip, fps=10, loop=False, autoplay=False, rd_kwargs={\"logger\": None}\n",
    "    )\n",
    "\n",
    "@torch.no_grad()\n",
    "def check_langevin_distribution(use_ibc_style):\n",
    "    seed(0)\n",
    "\n",
    "    mean = torch.tensor([0.3, 0.4])\n",
    "    std = torch.as_tensor([0.1, 0.2])\n",
    "\n",
    "    images = []\n",
    "    iteration_interval = max(1, n_iters // 10)\n",
    "\n",
    "    def callback(iteration, y_samples):\n",
    "        if iteration % iteration_interval != 0:\n",
    "            return\n",
    "        image = plot_2d_ebm_callback(ebm_net, iteration, x, y_samples)\n",
    "        images.append(image)\n",
    "\n",
    "    ebm_net = NormalEbm(mean, std)\n",
    "    ebm_net.eval()\n",
    "    # Observation `x` isn't really used here.\n",
    "    x = torch.zeros(size=(1, 0))\n",
    "    ys = langevin_sample(\n",
    "        ebm_net,\n",
    "        x,\n",
    "        num_samples,\n",
    "        callback=callback,\n",
    "        use_ibc_style=use_ibc_style,\n",
    "    )\n",
    "    ys = ys.squeeze(0)\n",
    "\n",
    "    # Check statistics.\n",
    "    std_actual, mean_actual = torch.std_mean(ys, dim=0, unbiased=False)\n",
    "    mean_rel_error = ((mean - mean_actual) / mean).abs().max()\n",
    "    std_rel_error = ((std - std_actual) / std).abs().max()\n",
    "    print(f\"mean_rel_error: {mean_rel_error}\")\n",
    "    print(f\"std_rel_error: {std_rel_error}\")\n",
    "    return mpy.ImageSequenceClip(_repeat_last(images), fps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2a16c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip = check_langevin_distribution(use_ibc_style=True)\n",
    "clip.write_videofile(\"/tmp/langevin_ibc_true.mp4\", logger=None)\n",
    "display_movie(clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc90292",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip = check_langevin_distribution(use_ibc_style=False)\n",
    "clip.write_videofile(\"/tmp/langevin_ibc_false.mp4\", logger=None)\n",
    "display_movie(clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d04582",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
