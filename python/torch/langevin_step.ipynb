{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "847116f6",
   "metadata": {},
   "source": [
    "Simplified setup and test for trivial energy function (anisotropic multivariate normal distribution).\n",
    "\n",
    "To run, go to this directory, run\n",
    "```\n",
    "source ./setup.sh \n",
    "jupyter lab ./langevin_step.ipynb\n",
    "```\n",
    "and execute all cells.\n",
    "\n",
    "If there are deps issues, try making a `virtualenv` given `requirements.freeze.txt`.\n",
    "\n",
    "Context:\n",
    " - <https://github.com/google-research/ibc/issues/6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c568e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses as dc\n",
    "import os\n",
    "import random\n",
    "\n",
    "import einops\n",
    "import matplotlib.pyplot as plt\n",
    "import moviepy\n",
    "import moviepy.editor as mpy\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd.functional import jacobian\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "from torch.distributions.uniform import Uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daeaddaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "DimY = 2\n",
    "\n",
    "\n",
    "@dc.dataclass\n",
    "class Config:\n",
    "    name: str\n",
    "    # How many iterations to take.\n",
    "    n_iters: int\n",
    "    # How many chains in parallel.\n",
    "    num_chains: int\n",
    "    # How many samples to keep along all chains.\n",
    "    history_count: int\n",
    "    # Interval to use for plotting.\n",
    "    iteration_interval: int\n",
    "    # Show first iteration (random init) in plotting.\n",
    "    show_first: bool\n",
    "\n",
    "    # Langevin.\n",
    "    y_min: torch.Tensor\n",
    "    y_max: torch.Tensor\n",
    "    y_mean: torch.Tensor\n",
    "    y_std: torch.Tensor\n",
    "    step_size_init: float\n",
    "    step_size_final: float\n",
    "    step_size_power: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0067f548-0f3b-4982-b974-6f5632a7805d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed(value):\n",
    "    random.seed(value)\n",
    "    np.random.seed(value)\n",
    "    torch.manual_seed(value)\n",
    "\n",
    "\n",
    "def uniform_sample(y_min, y_max, num_chains, batch_size):\n",
    "    lb = y_min.expand(batch_size, num_chains, DimY)\n",
    "    ub = y_max.expand(batch_size, num_chains, DimY)\n",
    "    return Uniform(lb, ub).sample()\n",
    "\n",
    "\n",
    "def gradient_wrt_act(ebm_net, x, ys):\n",
    "    \"\"\"Same as in google-research/ibc.\"\"\"\n",
    "    assert not torch.is_grad_enabled()\n",
    "\n",
    "    def Ex_sum(ys):\n",
    "        # Adapt trick from:\n",
    "        # https://discuss.pytorch.org/t/computing-batch-jacobian-efficiently/80771/5  # noqa\n",
    "        energies = ebm_net(x, ys)\n",
    "        return energies.sum()\n",
    "\n",
    "    # WARNING: This may be rather slow.\n",
    "    with torch.set_grad_enabled(True):\n",
    "        dE_dys = jacobian(Ex_sum, ys)\n",
    "    assert dE_dys.shape == ys.shape\n",
    "    return dE_dys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7276432-095b-4484-acb5-ecb30b1f5f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_step_size(config, iteration):\n",
    "    blend = iteration / (config.n_iters - 1)\n",
    "    blend = blend ** config.step_size_power\n",
    "    step_size = config.step_size_init + blend * (config.step_size_final - config.step_size_init)\n",
    "    return step_size\n",
    "\n",
    "\n",
    "def langevin_step(y_samples, dE_dys, step_size, use_ibc_style=True):\n",
    "    # Independent draw for covariance.\n",
    "    y_noise = torch.normal(\n",
    "        mean=0.0,\n",
    "        std=1.0,\n",
    "        size=y_samples.shape,\n",
    "        device=y_samples.device,\n",
    "    )\n",
    "    # Perturb samples according to gradient and desired noise level.\n",
    "    if use_ibc_style:\n",
    "        # From google-research/ibc.\n",
    "        delta_y = -step_size * (0.5 * dE_dys + y_noise)\n",
    "    else:\n",
    "        # Correct formulation. See:\n",
    "        # https://en.wikipedia.org/wiki/Metropolis-adjusted_Langevin_algorithm  # noqa\n",
    "        # Note that this uses different step size scaling.\n",
    "        delta_y = -step_size * 0.5 * dE_dys + np.sqrt(step_size) * y_noise\n",
    "    # Shift current actions\n",
    "    y_samples = y_samples + delta_y\n",
    "    return y_samples\n",
    "\n",
    "\n",
    "def langevin_sample(config, ebm_net, x, num_chains, callback=None, *, use_ibc_style=True):\n",
    "    assert not torch.is_grad_enabled()\n",
    "    N = x.shape[0]\n",
    "    # Draw initial samples.\n",
    "    y_samples = uniform_sample(config.y_min, config.y_max, num_chains, batch_size=N)\n",
    "    for i in range(config.n_iters + 1):\n",
    "        is_last = i == config.n_iters\n",
    "        if callback is not None:\n",
    "            callback(i, y_samples)\n",
    "        if not is_last:\n",
    "            # Compute gradient.\n",
    "            dE_dys = gradient_wrt_act(ebm_net, x, y_samples)\n",
    "            # Compute step size given current iteration.\n",
    "            step_size = get_step_size(config, i)\n",
    "            # Produce next set of samples (driving towards typical set).\n",
    "            y_samples = langevin_step(y_samples, dE_dys, step_size, use_ibc_style=use_ibc_style)\n",
    "    return y_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aeeb5a0-ab85-4c2a-9d95-16af3426d23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_log_normal_pdf(x, mu, std):\n",
    "    \"\"\"Log of multivariate normal distribution.\"\"\"\n",
    "    cov = torch.diag(std ** 2)\n",
    "    return MultivariateNormal(mu, cov).log_prob(x)\n",
    "\n",
    "\n",
    "class NormalEbm(nn.Module):\n",
    "    \"\"\"\n",
    "    Provides an energy function that represents a normal distribution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, y_mu, y_std):\n",
    "        super().__init__()\n",
    "        assert y_std.shape == y_mu.shape\n",
    "        self._y_std = y_std\n",
    "        self._y_mu = y_mu\n",
    "\n",
    "    def forward(self, x, ys):\n",
    "        N, K, _ = ys.shape\n",
    "        ys = einops.rearrange(ys, \"N K DimY -> (N K) DimY\")\n",
    "        # N.B. Incoprorate log into pdf computation so we avoid numeric\n",
    "        # problems in autograd.\n",
    "        probs = torch_log_normal_pdf(ys, self._y_mu, self._y_std)\n",
    "        probs = einops.rearrange(probs, \"(N K) -> N K 1\", N=N)\n",
    "        energies = -probs\n",
    "        return energies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55b07a9-d3e6-4f1b-a014-e3d2e38288b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def plot_2d_ebm(config, ebm_net, x, grid_size=200, alpha=None):\n",
    "    # We will denote unnormalized action coordinates as `u` and `v`.\n",
    "    action_u = torch.linspace(config.y_min[0], config.y_max[0], steps=grid_size)\n",
    "    action_v = torch.linspace(config.y_min[1], config.y_max[1], steps=grid_size)\n",
    "    action_v_grid, action_u_grid = torch.meshgrid(action_u, action_v)\n",
    "    action_us = einops.rearrange(action_u_grid, \"H W -> (H W)\")\n",
    "    action_vs = einops.rearrange(action_v_grid, \"H W -> (H W)\")\n",
    "    ys = einops.rearrange([action_us, action_vs], \"C HW -> () HW C\").to(x)\n",
    "    Zs = ebm_net(x, ys)\n",
    "    Z_grid = einops.rearrange(Zs, \"1 N 1 -> N\")\n",
    "    # Show probabilities used for sampling.\n",
    "    num_grid_samples = grid_size ** 2\n",
    "    Z_grid = torch.exp(-Z_grid)\n",
    "    Z_grid = einops.rearrange(Z_grid, \"(H W) -> H W\", H=grid_size)\n",
    "    mesh = plt.pcolormesh(\n",
    "        action_u_grid.numpy(),\n",
    "        action_v_grid.numpy(),\n",
    "        Z_grid.numpy(),\n",
    "        cmap=\"cool\",\n",
    "        alpha=alpha,\n",
    "        shading=\"auto\",\n",
    "    )\n",
    "    return mesh\n",
    "\n",
    "\n",
    "def plot_2d_ebm_callback(config, ebm_net, iteration, x, y_samples, *, for_pub=False):\n",
    "    N, _ = x.shape\n",
    "    assert N == 1\n",
    "    if for_pub:\n",
    "        fig, ax = plt.subplots(figsize=(3, 2))\n",
    "        alpha = 0.5\n",
    "    else:\n",
    "        fig, ax = plt.subplots()\n",
    "        alpha = 1.0\n",
    "    mesh = plot_2d_ebm(config, ebm_net, x)\n",
    "    plt.colorbar(mesh, label=r\"$p(y)$\")\n",
    "    plt.grid(False)\n",
    "    plt.axis(\"scaled\")\n",
    "    y_samples = y_samples.squeeze(0)\n",
    "    plt.scatter(\n",
    "        y_samples[:, 0], y_samples[:, 1], marker=\"x\", s=10, color=\"blue\", alpha=alpha\n",
    "    )\n",
    "    if for_pub:\n",
    "        return fig\n",
    "    else:\n",
    "        plt.title(f\"Iter {iteration} / {config.n_iters}\")\n",
    "        image = mpl_figure_to_image(fig)\n",
    "        plt.close(fig)\n",
    "        return image\n",
    "\n",
    "\n",
    "def repeat_last(images, *, count):\n",
    "    # To ensure last frame is saved.\n",
    "    assert len(images) > 0\n",
    "    return images + [images[-1]] * count\n",
    "\n",
    "\n",
    "def merge_rgba_to_rgb(rgba, *, bg_color=[255, 255, 255]):\n",
    "    assert rgba.dtype == np.uint8\n",
    "    rgb = rgba[..., :3]\n",
    "    alpha = rgba[..., [3]] / 255.0\n",
    "    rgb = alpha * rgb + bg_color * (1 - alpha)\n",
    "    return rgb.astype(np.uint8)\n",
    "\n",
    "\n",
    "def mpl_figure_to_image(fig):\n",
    "    fig.canvas.draw()\n",
    "    buffer = fig.canvas.buffer_rgba()\n",
    "    rgba = np.asarray(buffer)\n",
    "    rgb = merge_rgba_to_rgb(rgba)\n",
    "    return rgb\n",
    "\n",
    "\n",
    "def jupyter_movie(clip):\n",
    "    return moviepy.video.io.html_tools.ipython_display(\n",
    "        clip, fps=10, loop=False, autoplay=False, rd_kwargs={\"logger\": None}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552adb67-4624-4168-885a-17bbd8a6d08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def check_langevin_distribution(config, *, use_ibc_style):\n",
    "    seed(0)\n",
    "    ebm_net = NormalEbm(config.y_mean, config.y_std)\n",
    "    ebm_net.eval()\n",
    "    # Observation `x` isn't really used here.\n",
    "    x = torch.zeros(size=(1, 0))\n",
    "    # Callback state.\n",
    "    images = []\n",
    "    all_ys = []\n",
    "    final_fig = None\n",
    "\n",
    "    def callback(iteration, ys_latest):\n",
    "        nonlocal final_fig\n",
    "        all_ys.append(ys_latest)\n",
    "        ys_history = torch.cat(all_ys[-config.history_count:], dim=1)\n",
    "        ys = ys_history\n",
    "        if iteration % config.iteration_interval != 0:\n",
    "            return\n",
    "        if iteration == 0 and not config.show_first:\n",
    "            return\n",
    "        image = plot_2d_ebm_callback(config, ebm_net, iteration, x, ys)\n",
    "        images.append(image)\n",
    "        if iteration == config.n_iters:\n",
    "            final_fig = plot_2d_ebm_callback(config, ebm_net, iteration, x, ys, for_pub=True)\n",
    "\n",
    "    # Do sampling.\n",
    "    ys_latest = langevin_sample(\n",
    "        config,\n",
    "        ebm_net,\n",
    "        x,\n",
    "        config.num_chains,\n",
    "        callback=callback,\n",
    "        use_ibc_style=use_ibc_style,\n",
    "    )\n",
    "    ys_history = torch.cat(all_ys[-config.history_count:], dim=1)\n",
    "    ys = ys_history\n",
    "    # Check statistics.\n",
    "    ys = ys.squeeze(0)\n",
    "    y_std_actual, y_mean_actual = torch.std_mean(ys, dim=0, unbiased=False)\n",
    "    # n.b. bad numeric condition if expected is zero, but eh.\n",
    "    y_mean_rel_error = ((config.y_mean - y_mean_actual) / config.y_mean).abs().max()\n",
    "    y_std_rel_error = ((config.y_std - y_std_actual) / config.y_std).abs().max()\n",
    "    print(f\"  mean_rel_error: {y_mean_rel_error}\")\n",
    "    print(f\"  std_rel_error: {y_std_rel_error}\")\n",
    "    assert final_fig is not None\n",
    "    return images, final_fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6f5aa5-734e-4032-ae14-beab8f9af480",
   "metadata": {},
   "outputs": [],
   "source": [
    "common = dict(\n",
    "    y_min=torch.tensor([0.0, 0.0]),\n",
    "    y_max=torch.tensor([1.0, 1.0]),\n",
    "    y_mean=torch.tensor([0.3, 0.4]),\n",
    "    y_std=torch.as_tensor([0.1, 0.2]),\n",
    "    step_size_init=5e-3,\n",
    "    step_size_final=1e-3,\n",
    "    step_size_power=2,\n",
    ")\n",
    "\n",
    "num_particles = 400\n",
    "\n",
    "# Multiple chains, computing against final\n",
    "multi_chain = Config(\n",
    "    name=\"multi_chain\",\n",
    "    n_iters=50,\n",
    "    num_chains=num_particles,\n",
    "    history_count=1,\n",
    "    iteration_interval=1,\n",
    "    show_first=True,\n",
    "    **common\n",
    ")\n",
    "# Single long chain, computing against history (after burn-in stage).\n",
    "single_chain = Config(\n",
    "    name=\"single_chain\",\n",
    "    n_iters=1000,\n",
    "    num_chains=1,\n",
    "    history_count=num_particles,\n",
    "    iteration_interval=50,\n",
    "    show_first=True,\n",
    "    **common\n",
    ")\n",
    "\n",
    "configs = [\n",
    "    multi_chain,\n",
    "    single_chain,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458a53e6-cb23-4913-ad25-e2adee1a4755",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"/tmp/langevin\", exist_ok=True)\n",
    "for config in configs:    \n",
    "    print(f\"[{config.name}]\")\n",
    "    for use_ibc_style, suffix in zip([True, False], [\"ibc\", \"correct\"]):\n",
    "        print(suffix)\n",
    "        base = f\"/tmp/langevin/{config.name}-{suffix}\"\n",
    "        images, final_fig = check_langevin_distribution(\n",
    "            config,\n",
    "            use_ibc_style=use_ibc_style,\n",
    "        )\n",
    "        fps = 20\n",
    "        clip = mpy.ImageSequenceClip(repeat_last(images, count=30), fps=fps)\n",
    "        clip.write_videofile(f\"{base}.mp4\", fps=fps, logger=None)\n",
    "        clip.write_gif(f\"{base}.gif\", fps=fps, logger=None)\n",
    "        final_fig.savefig(f\"{base}-final.png\", dpi=300, bbox_inches=\"tight\")\n",
    "        display(jupyter_movie(clip))\n",
    "        display(final_fig)\n",
    "        plt.close(final_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07627269-b56f-4282-8e49-e69206a4d6af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
