{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "according-racing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from contextlib import contextmanager\n",
    "import random\n",
    "\n",
    "# TODO(eric.cousineau): Use tensorboard in notebook.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "particular-karma",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 1000\n",
    "pd.options.display.max_colwidth = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ambient-flood",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_grad(p):\n",
    "    if p.grad is not None:\n",
    "        p.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "champion-accent",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_loss(y, yh):\n",
    "    return torch.mean(torch.abs(y - yh))\n",
    "\n",
    "def mse_loss(y, yh):\n",
    "    return torch.mean((y - yh)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "global-hudson",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_cat_detached(ps):\n",
    "    return torch.cat([p.detach().view(-1) for p in ps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "contained-singing",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialDict(nn.Sequential):\n",
    "    \"\"\"\n",
    "    We must use OrderedDict because otherwise pytorch will sort the keys... I think?\n",
    "    See:\n",
    "        https://discuss.pytorch.org/t/append-for-nn-sequential-or-directly-converting-nn-modulelist-to-nn-sequential/7104/4\n",
    "        https://github.com/pytorch/pytorch/pull/40905\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(OrderedDict(*args, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "comfortable-trace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed(value):\n",
    "    random.seed(value)\n",
    "    np.random.seed(value)\n",
    "    torch.manual_seed(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "raising-disability",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"Simple Multi-Layer Perceptron.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_inputs,\n",
    "        num_outputs,\n",
    "        *,\n",
    "        num_hidden_units,\n",
    "        num_layers,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert num_layers >= 2\n",
    "        self.layers = SequentialDict()\n",
    "        self.layers.input = SequentialDict(\n",
    "            fcn=nn.Linear(num_inputs, num_hidden_units),\n",
    "            activation=nn.ReLU(),\n",
    "        )\n",
    "        hidden = []\n",
    "        for i in range(num_layers - 2):\n",
    "            hidden.append(SequentialDict(\n",
    "                fcn=nn.Linear(num_hidden_units, num_hidden_units),\n",
    "                activation=nn.ReLU(),\n",
    "            ))\n",
    "        self.layers.hidden = nn.Sequential(*hidden)\n",
    "        self.layers.output = SequentialDict(\n",
    "            fcn=nn.Linear(num_hidden_units, num_outputs)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "authorized-climb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derived from this kinda goal: https://towardsdatascience.com/how-to-visualize-convolutional-features-in-40-lines-of-code-70b7d87b0030\n",
    "class SaveActivations:\n",
    "    def __init__(self):\n",
    "        self.y = None\n",
    "\n",
    "    def forward_hook(self, module, x, y):\n",
    "        self.y = y.detach()\n",
    "\n",
    "@contextmanager\n",
    "def save_activations(module):\n",
    "    savers = []\n",
    "    hooks = []\n",
    "    for m in module.modules():\n",
    "        if isinstance(m, nn.ReLU):\n",
    "            saver = SaveActivations()\n",
    "            hook = m.register_forward_hook(saver.forward_hook)\n",
    "            savers.append(saver)\n",
    "            hooks.append(hook)\n",
    "    assert len(hooks) > 0\n",
    "    yield savers\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "\n",
    "def compute_activation_ratios(savers, clear=True):\n",
    "    activation_ratios = []\n",
    "    for saver in savers:\n",
    "        assert saver.y is not None\n",
    "        activation_ratios.append((saver.y > 0).to(torch.float).mean())\n",
    "        if clear:\n",
    "            saver.y = None\n",
    "    return torch.tensor(activation_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ethical-singing",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def shit_init_param(p, scale=1.0, offset=1e-5):\n",
    "    values = torch.linspace(-1.0, 1.0, p.numel()) / p.numel()\n",
    "    sign = torch.sign(values)\n",
    "    sign[sign == 0] = 1.0\n",
    "    values += offset * sign\n",
    "    p[:] = scale * values.reshape(p.shape)\n",
    "\n",
    "def shit_init(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            shit_init_param(m.weight)\n",
    "            shit_init_param(m.bias, scale=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "collected-covering",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters.\n",
    "amplitude = 1.0\n",
    "period_sec = 1.0\n",
    "shift_sec = 0.0\n",
    "num_periods = 1.0\n",
    "dt = 0.1\n",
    "count_per_period = int(np.ceil(period_sec / dt))\n",
    "count = num_periods * count_per_period\n",
    "t = torch.arange(count) * dt\n",
    "\n",
    "# def waveform(t):\n",
    "#     omega = 2 * np.pi / period_sec\n",
    "#     x = omega * (t + shift_sec)\n",
    "#     return amplitude * torch.sin(x)\n",
    "\n",
    "def waveform(t):\n",
    "    c = 2.0\n",
    "    return c * t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "greek-sandwich",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rel_mean_abs(x, tol=torch.tensor(1e-8)):\n",
    "    xa = x.abs()\n",
    "    xa_div = torch.fmax(xa.max(), tol)\n",
    "    xa_mean = xa.mean()\n",
    "    return xa_mean / xa_div\n",
    "\n",
    "def mean_abs(x):\n",
    "    return x.abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "operating-swaziland",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, loss_fn, lr, t, num_epochs=3, batch_size=1):\n",
    "    model.train()\n",
    "    # Expected param + labeled dataset.\n",
    "    t = t.unsqueeze(-1)\n",
    "    y = waveform(t)\n",
    "\n",
    "    dataset = list(zip(t, y))\n",
    "    loader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "    # Logging\n",
    "    dfs = []\n",
    "\n",
    "    with save_activations(model) as savers:\n",
    "        \n",
    "        def simple_log(epoch, batch_idx):\n",
    "            # Show err.\n",
    "            activation_ratios = compute_activation_ratios(savers).numpy()\n",
    "            activation_ratios_str = \", \".join([f\"{x:.2f}\" for x in activation_ratios])\n",
    "            dp = p - p_prev\n",
    "            dfs.append(pd.DataFrame(\n",
    "                {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"batch_idx\": batch_idx,\n",
    "                    \"mean|p|\": mean_abs(p).detach().numpy(),\n",
    "                    \"mean|Δp|\": mean_abs(dp).numpy(),\n",
    "                    \"loss\": loss.detach().numpy(),\n",
    "                    \"activation_ratios\": activation_ratios_str,\n",
    "                },\n",
    "                index=[len(dfs)],\n",
    "            ))\n",
    "\n",
    "        params = list(model.parameters())  # this is iter\n",
    "        p = flat_cat_detached(params)\n",
    "        p_prev = p.clone()\n",
    "        with torch.no_grad():\n",
    "            yh0 = model(t)\n",
    "            loss = loss_fn(y, yh0)\n",
    "        simple_log(epoch=\"pre-opt\", batch_idx=\"n/a\")\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            for batch_idx, (ti, yi) in enumerate(loader):\n",
    "                zero_grad(p)\n",
    "                yhi = model(ti)\n",
    "                loss = loss_fn(yi, yhi)\n",
    "                loss.backward()\n",
    "\n",
    "                # SGD update, no momentum.\n",
    "                with torch.no_grad():\n",
    "                    for param in params:\n",
    "                        v = param.grad  # velocity\n",
    "                        param -= lr * v  # step\n",
    "\n",
    "                # Validation.\n",
    "                p = flat_cat_detached(params)\n",
    "                simple_log(epoch, batch_idx)\n",
    "                p_prev = p\n",
    "\n",
    "    print(loss_fn)\n",
    "    display(pd.concat(dfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "finished-glucose",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model():\n",
    "    model = MLP(1, 1, num_hidden_units=20, num_layers=20)\n",
    "    shit_init(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "charitable-animation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function mse_loss at 0x7f213b9e2b70>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>batch_idx</th>\n",
       "      <th>mean|p|</th>\n",
       "      <th>mean|Δp|</th>\n",
       "      <th>loss</th>\n",
       "      <th>activation_ratios</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pre-opt</td>\n",
       "      <td>n/a</td>\n",
       "      <td>0.001476</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.328135</td>\n",
       "      <td>0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001518</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>1.328135</td>\n",
       "      <td>0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001631</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.684703</td>\n",
       "      <td>0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001739</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.332519</td>\n",
       "      <td>0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001782</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.788467</td>\n",
       "      <td>0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001740</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>1.372087</td>\n",
       "      <td>0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001745</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>1.209236</td>\n",
       "      <td>0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001751</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.550818</td>\n",
       "      <td>0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001749</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.371571</td>\n",
       "      <td>0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001693</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.945545</td>\n",
       "      <td>0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001913</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>1.132729</td>\n",
       "      <td>0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      epoch batch_idx   mean|p|  mean|Δp|      loss  \\\n",
       "0   pre-opt       n/a  0.001476  0.000000  1.328135   \n",
       "1         0         0  0.001518  0.000068  1.328135   \n",
       "2         1         0  0.001631  0.000113  0.684703   \n",
       "3         2         0  0.001739  0.000108  0.332519   \n",
       "4         3         0  0.001782  0.000067  0.788467   \n",
       "5         4         0  0.001740  0.000054  1.372087   \n",
       "6         5         0  0.001745  0.000125  1.209236   \n",
       "7         6         0  0.001751  0.000170  0.550818   \n",
       "8         7         0  0.001749  0.000144  0.371571   \n",
       "9         8         0  0.001693  0.000152  0.945545   \n",
       "10        9         0  0.001913  0.000238  1.132729   \n",
       "\n",
       "                                                                                                   activation_ratios  \n",
       "0   0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50  \n",
       "1   0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50  \n",
       "2   0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50  \n",
       "3   0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50  \n",
       "4   0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50  \n",
       "5   0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50  \n",
       "6   0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50  \n",
       "7   0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50  \n",
       "8   0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50  \n",
       "9   0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50  \n",
       "10  0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Merp\n",
    "fit(make_model(), loss_fn=mse_loss, lr=0.2, t=t, num_epochs=10, batch_size=len(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "published-uganda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
