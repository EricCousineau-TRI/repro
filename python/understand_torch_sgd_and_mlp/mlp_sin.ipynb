{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "popular-saturday",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from contextlib import contextmanager\n",
    "import random\n",
    "\n",
    "# TODO(eric.cousineau): Use tensorboard in notebook.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competent-arena",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extreme-killing",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 1000\n",
    "pd.options.display.max_colwidth = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-burton",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_grad(p):\n",
    "    if p.grad is not None:\n",
    "        p.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "combined-hawaii",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_loss(y, yh):\n",
    "    return torch.mean(torch.abs(y - yh))\n",
    "\n",
    "def mse_loss(y, yh):\n",
    "    return torch.mean((y - yh)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medieval-colors",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_cat_detached(ps):\n",
    "    return torch.cat([p.detach().view(-1) for p in ps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-reading",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialDict(nn.Sequential):\n",
    "    \"\"\"\n",
    "    We must use OrderedDict because otherwise pytorch will sort the keys... I think?\n",
    "    See:\n",
    "        https://discuss.pytorch.org/t/append-for-nn-sequential-or-directly-converting-nn-modulelist-to-nn-sequential/7104/4\n",
    "        https://github.com/pytorch/pytorch/pull/40905\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(OrderedDict(*args, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civilian-cursor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed(value):\n",
    "    random.seed(value)\n",
    "    np.random.seed(value)\n",
    "    torch.manual_seed(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "major-paragraph",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_cls = nn.ReLU\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"Simple Multi-Layer Perceptron.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_inputs,\n",
    "        num_outputs,\n",
    "        *,\n",
    "        num_hidden_units,\n",
    "        num_layers,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert num_layers >= 2\n",
    "        self.layers = SequentialDict()\n",
    "        self.layers.input = SequentialDict(\n",
    "            fcn=nn.Linear(num_inputs, num_hidden_units),\n",
    "            activation=activation_cls(),\n",
    "        )\n",
    "        hidden = []\n",
    "        for i in range(num_layers - 2):\n",
    "            hidden.append(SequentialDict(\n",
    "                fcn=nn.Linear(num_hidden_units, num_hidden_units),\n",
    "                activation=activation_cls(),\n",
    "            ))\n",
    "        self.layers.hidden = nn.Sequential(*hidden)\n",
    "        self.layers.output = SequentialDict(\n",
    "            fcn=nn.Linear(num_hidden_units, num_outputs)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executive-sydney",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derived from this kinda goal: https://towardsdatascience.com/how-to-visualize-convolutional-features-in-40-lines-of-code-70b7d87b0030\n",
    "class SaveActivations:\n",
    "    def __init__(self):\n",
    "        self.y = None\n",
    "\n",
    "    def forward_hook(self, module, x, y):\n",
    "        self.y = y.detach()\n",
    "\n",
    "@contextmanager\n",
    "def save_activations(module, cls):\n",
    "    savers = []\n",
    "    hooks = []\n",
    "    for m in module.modules():\n",
    "        if isinstance(m, cls):\n",
    "            saver = SaveActivations()\n",
    "            hook = m.register_forward_hook(saver.forward_hook)\n",
    "            savers.append(saver)\n",
    "            hooks.append(hook)\n",
    "    assert len(hooks) > 0\n",
    "    yield savers\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "\n",
    "def compute_activation_ratios(savers, clear=True):\n",
    "    activation_ratios = []\n",
    "    for saver in savers:\n",
    "        assert saver.y is not None\n",
    "        activated = (saver.y > 0)\n",
    "        assert activated.ndim > 1\n",
    "        N = activated.shape[0]\n",
    "        assert N > 1 # BESPOKE DBG HACK!!\n",
    "        # reduce along batch dimension using \"or\" (we want all act. at least somewhere in data)\n",
    "        # dunno how to do batch sum along \n",
    "        activated_reduce = torch.sum(activated.to(torch.int), axis=0).to(torch.bool)\n",
    "        activation_ratios.append(activated_reduce.to(torch.float).mean())\n",
    "        if clear:\n",
    "            saver.y = None\n",
    "    return torch.tensor(activation_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-alaska",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def shit_init_param(p, scale=1.0, offset=1e-5):\n",
    "    values = torch.linspace(-1.0, 1.0, p.numel()) / p.numel()\n",
    "    sign = torch.sign(values)\n",
    "    sign[sign == 0] = 1.0\n",
    "    values += offset * sign\n",
    "    p[:] = scale * values.reshape(p.shape)\n",
    "\n",
    "def shit_init(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            shit_init_param(m.weight)\n",
    "            shit_init_param(m.bias, scale=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-oliver",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I feel like an eediot.\n",
    "@torch.no_grad()\n",
    "def plot_pwa_via_relu():\n",
    "    t = torch.linspace(0, 4, 5)\n",
    "    y = F.relu(t) - 2 * F.relu(t - 1) + 2 * F.relu(t - 3)\n",
    "    plt.plot(t.numpy(), y.numpy(), linewidth=3)\n",
    "    \n",
    "    fc1 = nn.Linear(1, 3)\n",
    "    fc1.weight[:] = torch.tensor([1, 1, 1]).unsqueeze(-1)\n",
    "    fc1.bias[:] = torch.tensor([0, -1, -3])\n",
    "    act1 = nn.ReLU()\n",
    "    fc2 = nn.Linear(3, 1, bias=False)\n",
    "    fc2.weight[:] = torch.tensor([1, -2, 2])\n",
    "    \n",
    "    yh = t.unsqueeze(-1)\n",
    "    yh = fc1(yh)\n",
    "    yh = act1(yh)\n",
    "    yh = fc2(yh)\n",
    "    plt.plot(t.numpy(), yh.numpy(), linewidth=2, linestyle=\"--\")\n",
    "\n",
    "plot_pwa_via_relu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepted-python",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # # Parameters.\n",
    "    # amplitude = 1.0\n",
    "    # period_sec = 1.0\n",
    "    # shift_sec = 0.0\n",
    "    # num_periods = 1.0\n",
    "    # dt = 0.1\n",
    "    # count_per_period = int(np.ceil(period_sec / dt))\n",
    "    # count = num_periods * count_per_period\n",
    "    # t = torch.arange(count) * dt\n",
    "\n",
    "    # def waveform(t):\n",
    "    #     omega = 2 * np.pi / period_sec\n",
    "    #     x = omega * (t + shift_sec)\n",
    "    #     return amplitude * torch.sin(x)\n",
    "\n",
    "    t = torch.linspace(0, 4.0, 9)\n",
    "\n",
    "    # def waveform(t):\n",
    "    #     c = 2.0\n",
    "    #     return c * t\n",
    "\n",
    "def waveform(t):\n",
    "    # Simple sawtooth wave.\n",
    "    y = F.relu(t) - 2 * F.relu(t - 1) + 2 * F.relu(t - 3)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increasing-tradition",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t.numpy(), waveform(t).detach().numpy(), 'x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boring-australia",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def gt_model():\n",
    "    # Represent sawtooth w/ simple (but not unique) MLP.\n",
    "    model = MLP(num_inputs=1, num_outputs=1, num_hidden_units=3, num_layers=3)\n",
    "    model.layers.input.fcn.weight[:] = torch.tensor([1, 1, 1]).unsqueeze(-1)\n",
    "    model.layers.input.fcn.bias[:] = 0.0\n",
    "    model.layers.hidden[0].fcn.weight[:] = torch.eye(3)\n",
    "    model.layers.hidden[0].fcn.bias[:] = torch.tensor([0, -1, -3])\n",
    "    model.layers.output.fcn.weight[:] = torch.tensor([[1, -2, 2]])\n",
    "    model.layers.output.fcn.bias[:] = 0.0\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certified-feelings",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def plot_model(model):\n",
    "    y = model(t.unsqueeze(-1))\n",
    "    plt.plot(t.numpy(), y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-flooring",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_model(gt_model())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contemporary-plate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rel_mean_abs(x, tol=torch.tensor(1e-8)):\n",
    "    xa = x.abs()\n",
    "    xa_div = torch.fmax(xa.max(), tol)\n",
    "    xa_mean = xa.mean()\n",
    "    return xa_mean / xa_div\n",
    "\n",
    "def mean_abs(x):\n",
    "    return x.abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complimentary-premium",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, loss_fn, lr, t, num_epochs=3, batch_size=None):\n",
    "    model.train()\n",
    "    # Expected param + labeled dataset.\n",
    "    t = t.unsqueeze(-1)\n",
    "    y = waveform(t)\n",
    "\n",
    "    dataset = list(zip(t, y))\n",
    "    if batch_size is None:\n",
    "        # Go from SGD to simple GD.\n",
    "        batch_size = len(dataset)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "    # Logging\n",
    "    dfs = []\n",
    "\n",
    "    with save_activations(model, activation_cls) as savers:\n",
    "        \n",
    "        def simple_log(epoch, batch_idx):\n",
    "            # Show err.\n",
    "            activation_ratios = compute_activation_ratios(savers).numpy()\n",
    "            activation_ratios_str = \", \".join([f\"{x:.2f}\" for x in activation_ratios])\n",
    "            dp = p - p_prev\n",
    "            dfs.append(pd.DataFrame(\n",
    "                {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"batch_idx\": batch_idx,\n",
    "                    \"mean|p|\": mean_abs(p).detach().numpy(),\n",
    "                    \"mean|Î”p|\": mean_abs(dp).numpy(),\n",
    "                    \"loss\": loss.detach().numpy(),\n",
    "                    \"activation_ratios\": activation_ratios_str,\n",
    "                },\n",
    "                index=[len(dfs)],\n",
    "            ))\n",
    "\n",
    "        params = list(model.parameters())  # this is iter\n",
    "        p = flat_cat_detached(params)\n",
    "        p_prev = p.clone()\n",
    "        with torch.no_grad():\n",
    "            yh0 = model(t)\n",
    "            loss = loss_fn(y, yh0)\n",
    "        simple_log(epoch=\"pre-opt\", batch_idx=\"n/a\")\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            for batch_idx, (ti, yi) in enumerate(loader):\n",
    "                zero_grad(p)\n",
    "                yhi = model(ti)\n",
    "                loss = loss_fn(yi, yhi)\n",
    "                loss.backward()\n",
    "\n",
    "                # SGD update, no momentum.\n",
    "                with torch.no_grad():\n",
    "                    for param in params:\n",
    "                        v = param.grad  # velocity\n",
    "                        param -= lr * v  # step\n",
    "\n",
    "                # Validation.\n",
    "                # TODO(eric): Activation tracking across minibatches is weird.\n",
    "                # For now, should be OK w/ batch_size=len(dataset).\n",
    "                p = flat_cat_detached(params)\n",
    "                simple_log(epoch, batch_idx)\n",
    "                p_prev = p\n",
    "\n",
    "    print(loss_fn)\n",
    "    display(pd.concat(dfs))\n",
    "    with torch.no_grad():\n",
    "        yhf = model(t)\n",
    "    plt.plot(t.numpy(), y.numpy(), linewidth=3, linestyle=\"--\", color=\"g\")\n",
    "    plt.plot(t.numpy(), yhf.numpy(), color=\"k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peripheral-hollywood",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model():\n",
    "#     seed(0)\n",
    "    model = MLP(1, 1, num_hidden_units=3, num_layers=3)\n",
    "    shit_init(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twelve-dress",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check against gt.\n",
    "fit(gt_model(), loss_fn=mse_loss, lr=0.5, t=t, num_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-anxiety",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
