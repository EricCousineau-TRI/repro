{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-telephone",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from contextlib import contextmanager\n",
    "import random\n",
    "\n",
    "# TODO(eric.cousineau): Use tensorboard in notebook.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contemporary-forwarding",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 1000\n",
    "pd.options.display.max_colwidth = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greek-computer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_grad(p):\n",
    "    if p.grad is not None:\n",
    "        p.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swedish-hometown",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_loss(y, yh):\n",
    "    return torch.mean(torch.abs(y - yh))\n",
    "\n",
    "def mse_loss(y, yh):\n",
    "    return torch.mean((y - yh)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improving-cambodia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_cat_detached(ps):\n",
    "    return torch.cat([p.detach().view(-1) for p in ps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soviet-genius",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialDict(nn.Sequential):\n",
    "    \"\"\"\n",
    "    We must use OrderedDict because otherwise pytorch will sort the keys... I think?\n",
    "    See:\n",
    "        https://discuss.pytorch.org/t/append-for-nn-sequential-or-directly-converting-nn-modulelist-to-nn-sequential/7104/4\n",
    "        https://github.com/pytorch/pytorch/pull/40905\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(OrderedDict(*args, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "athletic-adapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed(value):\n",
    "    random.seed(value)\n",
    "    np.random.seed(value)\n",
    "    torch.manual_seed(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moral-dodge",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_cls = nn.ReLU\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"Simple Multi-Layer Perceptron.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_inputs,\n",
    "        num_outputs,\n",
    "        *,\n",
    "        num_hidden_units,\n",
    "        num_layers,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert num_layers >= 2\n",
    "        self.layers = SequentialDict()\n",
    "        self.layers.input = SequentialDict(\n",
    "            fcn=nn.Linear(num_inputs, num_hidden_units),\n",
    "            activation=activation_cls(),\n",
    "        )\n",
    "        hidden = []\n",
    "        for i in range(num_layers - 2):\n",
    "            hidden.append(SequentialDict(\n",
    "                fcn=nn.Linear(num_hidden_units, num_hidden_units),\n",
    "                activation=activation_cls(),\n",
    "            ))\n",
    "        self.layers.hidden = nn.Sequential(*hidden)\n",
    "        self.layers.output = SequentialDict(\n",
    "            fcn=nn.Linear(num_hidden_units, num_outputs)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "irish-ivory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derived from this kinda goal: https://towardsdatascience.com/how-to-visualize-convolutional-features-in-40-lines-of-code-70b7d87b0030\n",
    "class SaveActivations:\n",
    "    def __init__(self):\n",
    "        self.y = None\n",
    "\n",
    "    def forward_hook(self, module, x, y):\n",
    "        self.y = y.detach()\n",
    "\n",
    "@contextmanager\n",
    "def save_activations(module, cls):\n",
    "    savers = []\n",
    "    hooks = []\n",
    "    for m in module.modules():\n",
    "        if isinstance(m, cls):\n",
    "            saver = SaveActivations()\n",
    "            hook = m.register_forward_hook(saver.forward_hook)\n",
    "            savers.append(saver)\n",
    "            hooks.append(hook)\n",
    "    assert len(hooks) > 0\n",
    "    yield savers\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "\n",
    "def compute_activation_ratios(savers, clear=True):\n",
    "    activation_ratios = []\n",
    "    for saver in savers:\n",
    "        assert saver.y is not None\n",
    "        activation_ratios.append((saver.y > 0).to(torch.float).mean())\n",
    "        if clear:\n",
    "            saver.y = None\n",
    "    return torch.tensor(activation_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-temple",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def shit_init_param(p, scale=1.0, offset=1e-5):\n",
    "    values = torch.linspace(-1.0, 1.0, p.numel()) / p.numel()\n",
    "    sign = torch.sign(values)\n",
    "    sign[sign == 0] = 1.0\n",
    "    values += offset * sign\n",
    "    p[:] = scale * values.reshape(p.shape)\n",
    "\n",
    "def shit_init(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            shit_init_param(m.weight)\n",
    "            shit_init_param(m.bias, scale=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-plaza",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Parameters.\n",
    "# amplitude = 1.0\n",
    "# period_sec = 1.0\n",
    "# shift_sec = 0.0\n",
    "# num_periods = 1.0\n",
    "# dt = 0.1\n",
    "# count_per_period = int(np.ceil(period_sec / dt))\n",
    "# count = num_periods * count_per_period\n",
    "# t = torch.arange(count) * dt\n",
    "\n",
    "# def waveform(t):\n",
    "#     omega = 2 * np.pi / period_sec\n",
    "#     x = omega * (t + shift_sec)\n",
    "#     return amplitude * torch.sin(x)\n",
    "\n",
    "t = torch.linspace(0, 1.0, 3)\n",
    "\n",
    "def waveform(t):\n",
    "    c = 2.0\n",
    "    return c * t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surface-height",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rel_mean_abs(x, tol=torch.tensor(1e-8)):\n",
    "    xa = x.abs()\n",
    "    xa_div = torch.fmax(xa.max(), tol)\n",
    "    xa_mean = xa.mean()\n",
    "    return xa_mean / xa_div\n",
    "\n",
    "def mean_abs(x):\n",
    "    return x.abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-macro",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, loss_fn, lr, t, num_epochs=3, batch_size=1):\n",
    "    model.train()\n",
    "    # Expected param + labeled dataset.\n",
    "    t = t.unsqueeze(-1)\n",
    "    y = waveform(t)\n",
    "\n",
    "    dataset = list(zip(t, y))\n",
    "    loader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "    # Logging\n",
    "    dfs = []\n",
    "\n",
    "    with save_activations(model, activation_cls) as savers:\n",
    "        \n",
    "        def simple_log(epoch, batch_idx):\n",
    "            # Show err.\n",
    "            activation_ratios = compute_activation_ratios(savers).numpy()\n",
    "            activation_ratios_str = \", \".join([f\"{x:.2f}\" for x in activation_ratios])\n",
    "            dp = p - p_prev\n",
    "            dfs.append(pd.DataFrame(\n",
    "                {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"batch_idx\": batch_idx,\n",
    "                    \"mean|p|\": mean_abs(p).detach().numpy(),\n",
    "                    \"mean|Î”p|\": mean_abs(dp).numpy(),\n",
    "                    \"loss\": loss.detach().numpy(),\n",
    "                    \"activation_ratios\": activation_ratios_str,\n",
    "                },\n",
    "                index=[len(dfs)],\n",
    "            ))\n",
    "\n",
    "        params = list(model.parameters())  # this is iter\n",
    "        p = flat_cat_detached(params)\n",
    "        p_prev = p.clone()\n",
    "        with torch.no_grad():\n",
    "            yh0 = model(t)\n",
    "            loss = loss_fn(y, yh0)\n",
    "        simple_log(epoch=\"pre-opt\", batch_idx=\"n/a\")\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            for batch_idx, (ti, yi) in enumerate(loader):\n",
    "                zero_grad(p)\n",
    "                yhi = model(ti)\n",
    "                loss = loss_fn(yi, yhi)\n",
    "                loss.backward()\n",
    "\n",
    "                # SGD update, no momentum.\n",
    "                with torch.no_grad():\n",
    "                    for param in params:\n",
    "                        v = param.grad  # velocity\n",
    "                        param -= lr * v  # step\n",
    "\n",
    "                # Validation.\n",
    "                p = flat_cat_detached(params)\n",
    "                simple_log(epoch, batch_idx)\n",
    "                p_prev = p\n",
    "\n",
    "    print(loss_fn)\n",
    "    display(pd.concat(dfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preliminary-benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model():\n",
    "#     seed(0)\n",
    "    model = MLP(1, 1, num_hidden_units=2, num_layers=3)\n",
    "    shit_init(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contrary-franklin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merp\n",
    "fit(make_model(), loss_fn=mse_loss, lr=0.5, t=t, num_epochs=10, batch_size=len(t))  # GD, not really SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protecting-costa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-detector",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I feel like an eediot\n",
    "\n",
    "@torch.no_grad()\n",
    "def plot_pwa_via_relu():\n",
    "    t = torch.linspace(0, 4, 5)\n",
    "    y = F.relu(t) - 2 * F.relu(t - 1) + 2 * F.relu(t - 3)\n",
    "    plt.plot(t.numpy(), y.numpy(), linewidth=3)\n",
    "    \n",
    "    fc1 = nn.Linear(1, 3)\n",
    "    fc1.weight[:] = torch.tensor([1, 1, 1]).unsqueeze(-1)\n",
    "    fc1.bias[:] = torch.tensor([0, -1, -3])\n",
    "    act1 = nn.ReLU()\n",
    "    fc2 = nn.Linear(3, 1, bias=False)\n",
    "    fc2.weight[:] = torch.tensor([1, -2, 2])\n",
    "    \n",
    "    yh = t.unsqueeze(-1)\n",
    "    yh = fc1(yh)\n",
    "    yh = act1(yh)\n",
    "    yh = fc2(yh)\n",
    "    plt.plot(t.numpy(), yh.numpy(), linewidth=2, linestyle=\"--\")\n",
    "\n",
    "plot_pwa_via_relu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "homeless-daughter",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
